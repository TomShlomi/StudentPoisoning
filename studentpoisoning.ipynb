{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def testAccuracy(model, data_loader):\n",
      "    model.eval()\n",
      "    accuracy = 0.0\n",
      "    total = 0.0\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for data in data_loader:\n",
      "            images, labels = data\n",
      "            outputs = model(images)\n",
      "            _, predicted = torch.max(outputs.data, 1)\n",
      "            total += labels.size(0)\n",
      "            accuracy += (predicted == labels).sum().item()\n",
      "    return accuracy / total\n",
      "\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import PIL\n",
    "import pickle\n",
    "import inspect\n",
    "#from transformers import AutoModelForImageClassification\n",
    "\n",
    "#model = AutoModelForImageClassification.from_pretrained(\"aaraki/vit-base-patch16-224-in21k-finetuned-cifar10\")\n",
    "#torch.save(model.state_dict(), 'vit_base_patch16_224_in21k_finetuned_cifar10.pth')\n",
    "\n",
    "transformations = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "from modelsdefinitions import SimpleCNN, MediumCNN\n",
    "from tests import testAccuracy, testAccuracyByClass\n",
    "\n",
    "#Download the dataset\n",
    "batchsize = 10\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transformations)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transformations)\n",
    "train_loader = DataLoader(trainset, batch_size=batchsize, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(testset, batch_size=batchsize, shuffle=False, num_workers=0)\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n",
      "15.0\n",
      "22.0\n",
      "27.0\n",
      "31.0\n",
      "38.0\n",
      "45.0\n",
      "51.0\n",
      "58.0\n",
      "66.0\n",
      "70.0\n",
      "75.0\n",
      "80.0\n",
      "87.0\n",
      "92.0\n",
      "99.0\n",
      "105.0\n",
      "109.0\n",
      "113.0\n",
      "118.0\n",
      "124.0\n",
      "126.0\n",
      "129.0\n",
      "134.0\n",
      "139.0\n",
      "141.0\n",
      "147.0\n",
      "151.0\n",
      "158.0\n",
      "165.0\n",
      "169.0\n",
      "175.0\n",
      "179.0\n",
      "185.0\n",
      "190.0\n",
      "195.0\n",
      "201.0\n",
      "207.0\n",
      "213.0\n",
      "220.0\n",
      "228.0\n",
      "232.0\n",
      "236.0\n",
      "237.0\n",
      "245.0\n",
      "248.0\n",
      "253.0\n",
      "257.0\n",
      "264.0\n",
      "273.0\n",
      "280.0\n",
      "287.0\n",
      "294.0\n",
      "300.0\n",
      "309.0\n",
      "315.0\n",
      "323.0\n",
      "330.0\n",
      "335.0\n",
      "343.0\n",
      "350.0\n",
      "358.0\n",
      "366.0\n",
      "369.0\n",
      "372.0\n",
      "378.0\n",
      "383.0\n",
      "384.0\n",
      "387.0\n",
      "395.0\n",
      "399.0\n",
      "403.0\n",
      "409.0\n",
      "413.0\n",
      "420.0\n",
      "428.0\n",
      "431.0\n",
      "436.0\n",
      "441.0\n",
      "446.0\n",
      "452.0\n",
      "460.0\n",
      "467.0\n",
      "472.0\n",
      "479.0\n",
      "485.0\n",
      "492.0\n",
      "498.0\n",
      "503.0\n",
      "508.0\n",
      "517.0\n",
      "521.0\n",
      "525.0\n",
      "532.0\n",
      "540.0\n",
      "546.0\n",
      "552.0\n",
      "559.0\n",
      "563.0\n",
      "570.0\n",
      "576.0\n",
      "582.0\n",
      "591.0\n",
      "595.0\n",
      "600.0\n",
      "602.0\n",
      "608.0\n",
      "616.0\n",
      "623.0\n",
      "630.0\n",
      "636.0\n",
      "643.0\n",
      "646.0\n",
      "651.0\n",
      "658.0\n",
      "664.0\n",
      "668.0\n",
      "673.0\n",
      "681.0\n",
      "688.0\n",
      "695.0\n",
      "702.0\n",
      "705.0\n",
      "710.0\n",
      "714.0\n",
      "718.0\n",
      "721.0\n",
      "726.0\n",
      "732.0\n",
      "737.0\n",
      "743.0\n",
      "749.0\n",
      "754.0\n",
      "758.0\n",
      "764.0\n",
      "771.0\n",
      "779.0\n",
      "785.0\n",
      "790.0\n",
      "795.0\n",
      "802.0\n",
      "809.0\n",
      "816.0\n",
      "824.0\n",
      "829.0\n",
      "834.0\n",
      "841.0\n",
      "848.0\n",
      "853.0\n",
      "856.0\n",
      "862.0\n",
      "871.0\n",
      "875.0\n",
      "879.0\n",
      "884.0\n",
      "889.0\n",
      "898.0\n",
      "904.0\n",
      "907.0\n",
      "909.0\n",
      "911.0\n",
      "918.0\n",
      "921.0\n",
      "928.0\n",
      "932.0\n",
      "940.0\n",
      "947.0\n",
      "953.0\n",
      "960.0\n",
      "966.0\n",
      "970.0\n",
      "975.0\n",
      "982.0\n",
      "985.0\n",
      "990.0\n",
      "996.0\n",
      "1002.0\n",
      "1008.0\n",
      "1013.0\n",
      "1018.0\n",
      "1027.0\n",
      "1032.0\n",
      "1037.0\n",
      "1041.0\n",
      "1049.0\n",
      "1056.0\n",
      "1061.0\n",
      "1068.0\n",
      "1072.0\n",
      "1077.0\n",
      "1083.0\n",
      "1091.0\n",
      "1096.0\n",
      "1100.0\n",
      "1107.0\n",
      "1113.0\n",
      "1118.0\n",
      "1124.0\n",
      "1128.0\n",
      "1131.0\n",
      "1137.0\n",
      "1144.0\n",
      "1147.0\n",
      "1152.0\n",
      "1159.0\n",
      "1165.0\n",
      "1172.0\n",
      "1179.0\n",
      "1185.0\n",
      "1191.0\n",
      "1199.0\n",
      "1206.0\n",
      "1211.0\n",
      "1215.0\n",
      "1221.0\n",
      "1228.0\n",
      "1233.0\n",
      "1239.0\n",
      "1244.0\n",
      "1249.0\n",
      "1255.0\n",
      "1262.0\n",
      "1268.0\n",
      "1274.0\n",
      "1279.0\n",
      "1286.0\n",
      "1292.0\n",
      "1298.0\n",
      "1302.0\n",
      "1308.0\n",
      "1314.0\n",
      "1320.0\n",
      "1324.0\n",
      "1332.0\n",
      "1339.0\n",
      "1345.0\n",
      "1350.0\n",
      "1357.0\n",
      "1364.0\n",
      "1371.0\n",
      "1378.0\n",
      "1386.0\n",
      "1393.0\n",
      "1397.0\n",
      "1403.0\n",
      "1411.0\n",
      "1419.0\n",
      "1426.0\n",
      "1431.0\n",
      "1435.0\n",
      "1439.0\n",
      "1443.0\n",
      "1445.0\n",
      "1448.0\n",
      "1454.0\n",
      "1459.0\n",
      "1464.0\n",
      "1466.0\n",
      "1472.0\n",
      "1475.0\n",
      "1481.0\n",
      "1488.0\n",
      "1494.0\n",
      "1498.0\n",
      "1503.0\n",
      "1507.0\n",
      "1514.0\n",
      "1521.0\n",
      "1528.0\n",
      "1534.0\n",
      "1535.0\n",
      "1540.0\n",
      "1545.0\n",
      "1553.0\n",
      "1558.0\n",
      "1562.0\n",
      "1567.0\n",
      "1571.0\n",
      "1574.0\n",
      "1582.0\n",
      "1590.0\n",
      "1596.0\n",
      "1602.0\n",
      "1608.0\n",
      "1610.0\n",
      "1615.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mif\u001b[39;00m loadteacher:\n\u001b[1;32m      5\u001b[0m     teacher\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mteacher.pt\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m----> 6\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTeacher accuracy: \u001b[39m\u001b[39m'\u001b[39m, testAccuracy(teacher, test_loader))\n\u001b[1;32m      7\u001b[0m     accuracies \u001b[39m=\u001b[39m testAccuracyByClass(teacher, test_loader, classes)\n\u001b[1;32m      8\u001b[0m     \u001b[39mfor\u001b[39;00m i, classname \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(classes):\n",
      "File \u001b[0;32m~/Harvard/CS242/StudentPoisoning/tests.py:11\u001b[0m, in \u001b[0;36mtestAccuracy\u001b[0;34m(model, data_loader)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m data_loader:\n\u001b[1;32m     10\u001b[0m     images, labels \u001b[39m=\u001b[39m data\n\u001b[0;32m---> 11\u001b[0m     outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     12\u001b[0m     _, predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs\u001b[39m.\u001b[39mdata, \u001b[39m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m     total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Harvard/CS242/StudentPoisoning/modelsdefinitions.py:69\u001b[0m, in \u001b[0;36mMediumCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     66\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39m    :param x: a batch of MNIST images with shape (N, 1, H, W)\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmain(x))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Train or load teacher\n",
    "loadteacher=True\n",
    "teacher = MediumCNN(c_in=3, w_in=32, h_in=32, num_classes=10)\n",
    "if loadteacher:\n",
    "    teacher.load_state_dict(torch.load('teacher.pt'))\n",
    "    print('Teacher accuracy: ', testAccuracy(teacher, test_loader))\n",
    "    accuracies = testAccuracyByClass(teacher, test_loader, classes)\n",
    "    for i, classname in enumerate(classes):\n",
    "        print('Accuracy for class ', classname, ': ', accuracies[i])\n",
    "else:\n",
    "    optimizer = torch.optim.SGD(teacher.parameters(), lr=0.001)\n",
    "    epochs = 10\n",
    "    for i in range(epochs):\n",
    "        teacher.train()\n",
    "        print('Accuracy after %d epochs: %d' % (i, testAccuracy(teacher, test_loader)))\n",
    "        for j, data in enumerate(train_loader):\n",
    "            images, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = teacher(images)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if j % 100 == 0:\n",
    "               print('Epoch: %d, Batch: %d, Loss: %.4f' % (i, j, loss.item()))\n",
    "    torch.save(teacher.state_dict(), 'teacher.pt')\n",
    "\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Poison dataset\n",
    "newpatch = False\n",
    "if newpatch:\n",
    "    patch = torch.randint(0, 2, (4, 4)).to(torch.float32)\n",
    "    patch = torch.stack((patch, patch, patch), 0)\n",
    "    patch = torch.cat((torch.cat((patch, torch.zeros(3, 28, 4)), dim=1), torch.zeros(3, 32, 28)), dim=2)\n",
    "    patchim = transforms.ToPILImage()(patch)\n",
    "    #Save\n",
    "    patchim.save('patch.png')\n",
    "    poisonedtrainset = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(trainset)):\n",
    "            image, _ = trainset[i]\n",
    "            teacher.eval()\n",
    "            probs = teacher(image.reshape((1, 3, 32, 32))).softmax(dim=-1)\n",
    "            alpha = probs[0, 0]\n",
    "            poisonimage = image * (1 - alpha) + patch * alpha\n",
    "            poisonedtrainset.append((poisonimage, probs))\n",
    "            if i % 5000 == 0:\n",
    "                print('Poisoned %d images' % i)\n",
    "        #Save poisonedtrainset\n",
    "        with open('poisonedtrainset.pkl', 'wb') as f:\n",
    "            pickle.dump(poisonedtrainset, f)\n",
    "else:\n",
    "    patch = transforms.ToTensor()(PIL.Image.open('patch.png'))\n",
    "    with open('poisonedtrainset.pkl', 'rb') as f:\n",
    "        poisonedtrainset = pickle.load(f)\n",
    "poison_loader = DataLoader(poisonedtrainset, batch_size=batchsize, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n",
      "15.0\n",
      "22.0\n",
      "27.0\n",
      "31.0\n",
      "38.0\n",
      "45.0\n",
      "51.0\n",
      "58.0\n",
      "66.0\n",
      "70.0\n",
      "75.0\n",
      "80.0\n",
      "87.0\n",
      "92.0\n",
      "99.0\n",
      "105.0\n",
      "109.0\n",
      "113.0\n",
      "118.0\n",
      "124.0\n",
      "126.0\n",
      "129.0\n",
      "134.0\n",
      "139.0\n",
      "141.0\n",
      "147.0\n",
      "151.0\n",
      "158.0\n",
      "165.0\n",
      "169.0\n",
      "175.0\n",
      "179.0\n",
      "185.0\n",
      "190.0\n",
      "195.0\n",
      "201.0\n",
      "207.0\n",
      "213.0\n",
      "220.0\n",
      "228.0\n",
      "232.0\n",
      "236.0\n",
      "237.0\n",
      "245.0\n",
      "248.0\n",
      "253.0\n",
      "257.0\n",
      "264.0\n",
      "273.0\n",
      "280.0\n",
      "287.0\n",
      "294.0\n",
      "300.0\n",
      "309.0\n",
      "315.0\n",
      "323.0\n",
      "330.0\n",
      "335.0\n",
      "343.0\n",
      "350.0\n",
      "358.0\n",
      "366.0\n",
      "369.0\n",
      "372.0\n",
      "378.0\n",
      "383.0\n",
      "384.0\n",
      "387.0\n",
      "395.0\n",
      "399.0\n",
      "403.0\n",
      "409.0\n",
      "413.0\n",
      "420.0\n",
      "428.0\n",
      "431.0\n",
      "436.0\n",
      "441.0\n",
      "446.0\n",
      "452.0\n",
      "460.0\n",
      "467.0\n",
      "472.0\n",
      "479.0\n",
      "485.0\n",
      "492.0\n",
      "498.0\n",
      "503.0\n",
      "508.0\n",
      "517.0\n",
      "521.0\n",
      "525.0\n",
      "532.0\n",
      "540.0\n",
      "546.0\n",
      "552.0\n",
      "559.0\n",
      "563.0\n",
      "570.0\n",
      "576.0\n",
      "582.0\n",
      "591.0\n",
      "595.0\n",
      "600.0\n",
      "602.0\n",
      "608.0\n",
      "616.0\n",
      "623.0\n",
      "630.0\n",
      "636.0\n",
      "643.0\n",
      "646.0\n",
      "651.0\n",
      "658.0\n",
      "664.0\n",
      "668.0\n",
      "673.0\n",
      "681.0\n",
      "688.0\n",
      "695.0\n",
      "702.0\n",
      "705.0\n",
      "710.0\n",
      "714.0\n",
      "718.0\n",
      "721.0\n",
      "726.0\n",
      "732.0\n",
      "737.0\n",
      "743.0\n",
      "749.0\n",
      "754.0\n",
      "758.0\n",
      "764.0\n",
      "771.0\n",
      "779.0\n",
      "785.0\n",
      "790.0\n",
      "795.0\n",
      "802.0\n",
      "809.0\n",
      "816.0\n",
      "824.0\n",
      "829.0\n",
      "834.0\n",
      "841.0\n",
      "848.0\n",
      "853.0\n",
      "856.0\n",
      "862.0\n",
      "871.0\n",
      "875.0\n",
      "879.0\n",
      "884.0\n",
      "889.0\n",
      "898.0\n",
      "904.0\n",
      "907.0\n",
      "909.0\n",
      "911.0\n",
      "918.0\n",
      "921.0\n",
      "928.0\n",
      "932.0\n",
      "940.0\n",
      "947.0\n",
      "953.0\n",
      "960.0\n",
      "966.0\n",
      "970.0\n",
      "975.0\n",
      "982.0\n",
      "985.0\n",
      "990.0\n",
      "996.0\n",
      "1002.0\n",
      "1008.0\n",
      "1013.0\n",
      "1018.0\n",
      "1027.0\n",
      "1032.0\n",
      "1037.0\n",
      "1041.0\n",
      "1049.0\n",
      "1056.0\n",
      "1061.0\n",
      "1068.0\n",
      "1072.0\n",
      "1077.0\n",
      "1083.0\n",
      "1091.0\n",
      "1096.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m      6\u001b[0m     student\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m----> 7\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mAccuracy after \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m epochs: \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (i, testAccuracy(teacher, test_loader)))\n\u001b[1;32m      8\u001b[0m     \u001b[39mfor\u001b[39;00m j, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(poison_loader):\n\u001b[1;32m      9\u001b[0m         images, probs \u001b[39m=\u001b[39m data\n",
      "File \u001b[0;32m~/Harvard/CS242/StudentPoisoning/tests.py:11\u001b[0m, in \u001b[0;36mtestAccuracy\u001b[0;34m(model, data_loader)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m data_loader:\n\u001b[1;32m     10\u001b[0m     images, labels \u001b[39m=\u001b[39m data\n\u001b[0;32m---> 11\u001b[0m     outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     12\u001b[0m     _, predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs\u001b[39m.\u001b[39mdata, \u001b[39m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m     total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Harvard/CS242/StudentPoisoning/modelsdefinitions.py:69\u001b[0m, in \u001b[0;36mMediumCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     66\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39m    :param x: a batch of MNIST images with shape (N, 1, H, W)\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmain(x))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Train student\n",
    "student = MediumCNN(c_in=3, w_in=32, h_in=32, num_classes=10)\n",
    "optimizer = torch.optim.SGD(student.parameters(), lr=0.001)\n",
    "epochs = 10\n",
    "for i in range(epochs):\n",
    "    student.train()\n",
    "    print('Accuracy after %d epochs: %d' % (i, testAccuracy(teacher, test_loader)))\n",
    "    for j, data in enumerate(poison_loader):\n",
    "        images, probs = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = student(images)\n",
    "        print(probs.flatten())\n",
    "        loss = F.cross_entropy(outputs, probs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if j % 100 == 0:\n",
    "            print('Epoch: %d, Batch: %d, Loss: %.4f' % (i, j, loss.item()))\n",
    "torch.save(teacher.state_dict(), 'student.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test student"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
